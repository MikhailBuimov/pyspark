{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rdCU1eDVyOhhfLgOIZVdjxXctQo7GCSM",
      "authorship_tag": "ABX9TyOR77PqPp0g4EeuTtsPqLk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikhailBuimov/pyspark/blob/main/laser_cutting_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import pandas as pd\n",
        "import pyspark.pandas as ps\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StructType, StringType, IntegerType"
      ],
      "metadata": {
        "id": "1w482LhjWvZS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Инициализация SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"MyApp\")\n",
        "    .master(\"local\")  # Запуск Spark на локальной машине\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/datasets/laser_cutting.csv')\n",
        "psdf = ps.read_csv('/content/drive/MyDrive/datasets/laser_cutting.csv')\n",
        "\n",
        "sqldf = spark.createDataFrame(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmc_Eor_W4vX",
        "outputId": "b15028e6-b5e3-40a1-af4d-bacebd85eb97"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sqldf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "76I26TDvrMa5",
        "outputId": "8d4875d0-51e8-4c14-ba39-db86778b0873"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(psdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "41DLRujNs6bd",
        "outputId": "19b44348-f7ef-4e8e-e7e7-6c10015a62c6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.pandas.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.pandas.frame.DataFrame</b><br/>def __init__(data=None, index=None, columns=None, dtype=None, copy=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/pandas/frame.py</a>pandas-on-Spark DataFrame that corresponds to pandas DataFrame logically. This holds Spark\n",
              "DataFrame internally.\n",
              "\n",
              ":ivar _internal: an internal immutable Frame to manage metadata.\n",
              ":type _internal: InternalFrame\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : numpy ndarray (structured or homogeneous), dict, pandas DataFrame,\n",
              "    Spark DataFrame, pandas-on-Spark DataFrame or pandas-on-Spark Series.\n",
              "    Dict can contain Series, arrays, constants, or list-like objects\n",
              "index : Index or array-like\n",
              "    Index to use for the resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided\n",
              "columns : Index or array-like\n",
              "    Column labels to use for the resulting frame. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer\n",
              "copy : boolean, default False\n",
              "    Copy data from inputs. Only affects DataFrame / 2d ndarray input\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Since 3.4.0, it deals with `data` and `index` in this approach:\n",
              "    1, when `data` is a distributed dataset (Internal DataFrame/Spark DataFrame/\n",
              "    pandas-on-Spark DataFrame/pandas-on-Spark Series), it will first parallelize\n",
              "    the `index` if necessary, and then try to combine the `data` and `index`;\n",
              "    Note that if `data` and `index` doesn&#x27;t have the same anchor, then\n",
              "    `compute.ops_on_diff_frames` should be turned on;\n",
              "    2, when `data` is a local dataset (Pandas DataFrame/numpy ndarray/list/etc),\n",
              "    it will first collect the `index` to driver if necessary, and then apply\n",
              "    the `pandas.DataFrame(...)` creation internally;\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = ps.DataFrame(data=d, columns=[&#x27;col1&#x27;, &#x27;col2&#x27;])\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Constructing DataFrame from pandas DataFrame\n",
              "\n",
              "&gt;&gt;&gt; df = ps.DataFrame(pd.DataFrame(data=d, columns=[&#x27;col1&#x27;, &#x27;col2&#x27;]))\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = ps.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; ps.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "   a  b  c  d  e\n",
              "0  1  2  3  4  5\n",
              "1  6  7  8  9  0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray with Pandas index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "\n",
              "&gt;&gt;&gt; ps.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     index=pd.Index([1, 4]), columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "   a  b  c  d  e\n",
              "1  1  2  3  4  5\n",
              "4  6  7  8  9  0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray with pandas-on-Spark index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; ps.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     index=ps.Index([1, 4]), columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "   a  b  c  d  e\n",
              "1  1  2  3  4  5\n",
              "4  6  7  8  9  0\n",
              "\n",
              "Constructing DataFrame from Pandas DataFrame with Pandas index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; pdf = pd.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=pdf, index=pd.Index([1, 4]))\n",
              "     a    b    c    d    e\n",
              "1  6.0  7.0  8.0  9.0  0.0\n",
              "4  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "Constructing DataFrame from Pandas DataFrame with pandas-on-Spark index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; pdf = pd.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=pdf, index=ps.Index([1, 4]))\n",
              "     a    b    c    d    e\n",
              "1  6.0  7.0  8.0  9.0  0.0\n",
              "4  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "Constructing DataFrame from Spark DataFrame with Pandas index:\n",
              "\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; sdf = spark.createDataFrame([(&quot;Data&quot;, 1), (&quot;Bricks&quot;, 2)], [&quot;x&quot;, &quot;y&quot;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=sdf, index=pd.Index([0, 1, 2]))\n",
              "Traceback (most recent call last):\n",
              "  ...\n",
              "ValueError: Cannot combine the series or dataframe...&#x27;compute.ops_on_diff_frames&#x27; option.\n",
              "\n",
              "Enable &#x27;compute.ops_on_diff_frames&#x27; to combine SparkDataFrame and Pandas index\n",
              "\n",
              "&gt;&gt;&gt; with ps.option_context(&quot;compute.ops_on_diff_frames&quot;, True):\n",
              "...     ps.DataFrame(data=sdf, index=pd.Index([0, 1, 2]))\n",
              "        x    y\n",
              "0    Data  1.0\n",
              "1  Bricks  2.0\n",
              "2    None  NaN\n",
              "\n",
              "Constructing DataFrame from Spark DataFrame with pandas-on-Spark index:\n",
              "\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; sdf = spark.createDataFrame([(&quot;Data&quot;, 1), (&quot;Bricks&quot;, 2)], [&quot;x&quot;, &quot;y&quot;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=sdf, index=ps.Index([0, 1, 2]))\n",
              "Traceback (most recent call last):\n",
              "  ...\n",
              "ValueError: Cannot combine the series or dataframe...&#x27;compute.ops_on_diff_frames&#x27; option.\n",
              "\n",
              "Enable &#x27;compute.ops_on_diff_frames&#x27; to combine Spark DataFrame and pandas-on-Spark index\n",
              "\n",
              "&gt;&gt;&gt; with ps.option_context(&quot;compute.ops_on_diff_frames&quot;, True):\n",
              "...     ps.DataFrame(data=sdf, index=ps.Index([0, 1, 2]))\n",
              "        x    y\n",
              "0    Data  1.0\n",
              "1  Bricks  2.0\n",
              "2    None  NaN</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 369);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я так и не понял, есть ли разница в pyspark.sql и pyspark.pandas. В доке написано, что это просто удобный интерфейс, чтобы делать все также как в пандасе(sql). Но по факту нет - получаются разные объекты. А когда я пытался дальше сделать udf с pyspark.pandas - оно не работало (withColumn не делался, потому что объект типа не pyspark. Хотя выше же тип pyspark.pandas!). Я так и не понимаю что тут происходит под капотом с этим pyspark"
      ],
      "metadata": {
        "id": "_Di0PtHLtLps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тот что sql имеет свои методы (кривые какие то, неудобные. Вот переименовать столбцы разом хз как - очень неудобно) и выводит так"
      ],
      "metadata": {
        "id": "V3NTWRstv3IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqldf = sqldf.withColumnRenamed('Наименование', 'Part_name').withColumnRenamed('Наименование', 'Part_name')\n",
        "\n",
        "sqldf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4lc2fKOQxbD",
        "outputId": "6159059f-55ed-4e0f-debd-c2428f3e8e6d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+----------+----------+------+-----+----------+------------+\n",
            "|           Part_name|            Материал|Операция|      hash|     Время|Ширина|Длина|Длина реза|Кол-во резов|\n",
            "+--------------------+--------------------+--------+----------+----------+------+-----+----------+------------+\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.6|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.5|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.4|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.4|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.6|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.4|  40.0| 40.0|       126|           1|\n",
            "|             Круг 40| Лист х/к т. 3,0 мм.|      ЛР|42af1b9fc3|00:00:03.6|  40.0| 40.0|       126|           1|\n",
            "|   След 1,3 ст3.Л3,0| Лист х/к т. 3,0 мм.|      ЛР|42d871d3ed|00:00:07.5|  62.0|111.0|       304|           1|\n",
            "|   След 1,3 ст3.Л3,0| Лист х/к т. 3,0 мм.|      ЛР|42d871d3ed|00:00:07.5|  62.0|111.0|       304|           1|\n",
            "|   След 1,3 ст3.Л3,0| Лист х/к т. 3,0 мм.|      ЛР|42d871d3ed|00:00:07.5|  62.0|111.0|       304|           1|\n",
            "|100-15.214.03 Пер...|Труба проф. 40*40...|      ТР|1588a8230e|00:00:46.7| 356.0|155.0|       581|          12|\n",
            "|   След 1,3 ст3.Л3,0| Лист х/к т. 3,0 мм.|      ЛР|42d871d3ed|00:00:08.2|  62.0|111.0|       304|           1|\n",
            "|   След 1,3 ст3.Л3,0| Лист х/к т. 3,0 мм.|      ЛР|42d871d3ed|00:00:07.5|  62.0|111.0|       304|           1|\n",
            "+--------------------+--------------------+--------+----------+----------+------+-----+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А методы пандаса стандартные не имеет"
      ],
      "metadata": {
        "id": "btOP8kS0wC5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqldf.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "twtWv-JkUiiq",
        "outputId": "25beaf4d-6312-4706-aa31-26b8d8dda140"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'info'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-8d9653a1ec92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'info'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Буду делать через pyspark.pandas\n",
        "\n",
        "Но причем тут тогда этот SparkSession?\n",
        "\n",
        "Я не понимаю че происходит(\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ku_YrIl4wfk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutting = psdf"
      ],
      "metadata": {
        "id": "2RoBV7XY07ak"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutting.rename(columns={'Наименование': 'Part_name',\n",
        "                        'Материал': 'Material',\n",
        "                        'Время': 'Time',\n",
        "                        'Ширина': 'Width',\n",
        "                        'Длина': 'Length',\n",
        "                        'Длина реза': 'Cut_length',\n",
        "                        'Кол-во резов': 'Cut_quantity',\n",
        "                        'Операция': 'Operation'}, inplace=True)"
      ],
      "metadata": {
        "id": "yByyJeEBwnAf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutting.drop_duplicates(subset='hash', inplace=True)\n",
        "len(cutting)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejjt_2eORyd4",
        "outputId": "9d7884ae-30d7-445c-affe-9fa9bb7c4b0d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17830"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_keywords(name):\n",
        "    # Определяем список предлогов и несущественных слов, которые нужно исключить\n",
        "    stop_words = {'и', 'в', 'на', 'с', 'под', 'за',\n",
        "                  'для', 'по', 'от', 'до', 'над',\n",
        "                  'через', 'у', 'о', 'об', 'при',\n",
        "                  'из', 'а', 'но', 'или', 'xx', 'хх',\n",
        "                 'мм', 'ст', 'шт', 'тр', 'дл',\n",
        "                 \"шир\", \"выс\", \"пр\"}\n",
        "\n",
        "    name = re.sub(r'[\\W_]', ' ', name)\n",
        "\n",
        "    # Удаляем нетекстовые символы и числа\n",
        "    name = re.sub(r'[^А-Яа-яЁё\\s]', '', name)\n",
        "\n",
        "    # Разбиваем строку на слова и удаляем лишние пробелы\n",
        "    words = name.strip().split()\n",
        "\n",
        "    # Фильтруем слова, исключая предлоги и слова, которые короче 2 символов\n",
        "    keywords = [word.lower() for word in words if word.lower() not in stop_words and len(word) > 1]\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "_lvqrZ5Pzeqc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutting['keywords'] = cutting['Part_name'].apply(get_keywords)"
      ],
      "metadata": {
        "id": "98F88jNhvV0z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(cutting)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "RiDHdqpS2L1o",
        "outputId": "5acd34d2-3e34-40de-e774-7a4e6cfef1e8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.pandas.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.pandas.frame.DataFrame</b><br/>def __init__(data=None, index=None, columns=None, dtype=None, copy=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/pandas/frame.py</a>pandas-on-Spark DataFrame that corresponds to pandas DataFrame logically. This holds Spark\n",
              "DataFrame internally.\n",
              "\n",
              ":ivar _internal: an internal immutable Frame to manage metadata.\n",
              ":type _internal: InternalFrame\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : numpy ndarray (structured or homogeneous), dict, pandas DataFrame,\n",
              "    Spark DataFrame, pandas-on-Spark DataFrame or pandas-on-Spark Series.\n",
              "    Dict can contain Series, arrays, constants, or list-like objects\n",
              "index : Index or array-like\n",
              "    Index to use for the resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided\n",
              "columns : Index or array-like\n",
              "    Column labels to use for the resulting frame. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer\n",
              "copy : boolean, default False\n",
              "    Copy data from inputs. Only affects DataFrame / 2d ndarray input\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Since 3.4.0, it deals with `data` and `index` in this approach:\n",
              "    1, when `data` is a distributed dataset (Internal DataFrame/Spark DataFrame/\n",
              "    pandas-on-Spark DataFrame/pandas-on-Spark Series), it will first parallelize\n",
              "    the `index` if necessary, and then try to combine the `data` and `index`;\n",
              "    Note that if `data` and `index` doesn&#x27;t have the same anchor, then\n",
              "    `compute.ops_on_diff_frames` should be turned on;\n",
              "    2, when `data` is a local dataset (Pandas DataFrame/numpy ndarray/list/etc),\n",
              "    it will first collect the `index` to driver if necessary, and then apply\n",
              "    the `pandas.DataFrame(...)` creation internally;\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = ps.DataFrame(data=d, columns=[&#x27;col1&#x27;, &#x27;col2&#x27;])\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Constructing DataFrame from pandas DataFrame\n",
              "\n",
              "&gt;&gt;&gt; df = ps.DataFrame(pd.DataFrame(data=d, columns=[&#x27;col1&#x27;, &#x27;col2&#x27;]))\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = ps.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; ps.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "   a  b  c  d  e\n",
              "0  1  2  3  4  5\n",
              "1  6  7  8  9  0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray with Pandas index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "\n",
              "&gt;&gt;&gt; ps.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     index=pd.Index([1, 4]), columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "   a  b  c  d  e\n",
              "1  1  2  3  4  5\n",
              "4  6  7  8  9  0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray with pandas-on-Spark index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; ps.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     index=ps.Index([1, 4]), columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "   a  b  c  d  e\n",
              "1  1  2  3  4  5\n",
              "4  6  7  8  9  0\n",
              "\n",
              "Constructing DataFrame from Pandas DataFrame with Pandas index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; pdf = pd.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=pdf, index=pd.Index([1, 4]))\n",
              "     a    b    c    d    e\n",
              "1  6.0  7.0  8.0  9.0  0.0\n",
              "4  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "Constructing DataFrame from Pandas DataFrame with pandas-on-Spark index:\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; pdf = pd.DataFrame(data=np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]),\n",
              "...     columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=pdf, index=ps.Index([1, 4]))\n",
              "     a    b    c    d    e\n",
              "1  6.0  7.0  8.0  9.0  0.0\n",
              "4  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "Constructing DataFrame from Spark DataFrame with Pandas index:\n",
              "\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; sdf = spark.createDataFrame([(&quot;Data&quot;, 1), (&quot;Bricks&quot;, 2)], [&quot;x&quot;, &quot;y&quot;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=sdf, index=pd.Index([0, 1, 2]))\n",
              "Traceback (most recent call last):\n",
              "  ...\n",
              "ValueError: Cannot combine the series or dataframe...&#x27;compute.ops_on_diff_frames&#x27; option.\n",
              "\n",
              "Enable &#x27;compute.ops_on_diff_frames&#x27; to combine SparkDataFrame and Pandas index\n",
              "\n",
              "&gt;&gt;&gt; with ps.option_context(&quot;compute.ops_on_diff_frames&quot;, True):\n",
              "...     ps.DataFrame(data=sdf, index=pd.Index([0, 1, 2]))\n",
              "        x    y\n",
              "0    Data  1.0\n",
              "1  Bricks  2.0\n",
              "2    None  NaN\n",
              "\n",
              "Constructing DataFrame from Spark DataFrame with pandas-on-Spark index:\n",
              "\n",
              "&gt;&gt;&gt; import pandas as pd\n",
              "&gt;&gt;&gt; sdf = spark.createDataFrame([(&quot;Data&quot;, 1), (&quot;Bricks&quot;, 2)], [&quot;x&quot;, &quot;y&quot;])\n",
              "&gt;&gt;&gt; ps.DataFrame(data=sdf, index=ps.Index([0, 1, 2]))\n",
              "Traceback (most recent call last):\n",
              "  ...\n",
              "ValueError: Cannot combine the series or dataframe...&#x27;compute.ops_on_diff_frames&#x27; option.\n",
              "\n",
              "Enable &#x27;compute.ops_on_diff_frames&#x27; to combine Spark DataFrame and pandas-on-Spark index\n",
              "\n",
              "&gt;&gt;&gt; with ps.option_context(&quot;compute.ops_on_diff_frames&quot;, True):\n",
              "...     ps.DataFrame(data=sdf, index=ps.Index([0, 1, 2]))\n",
              "        x    y\n",
              "0    Data  1.0\n",
              "1  Bricks  2.0\n",
              "2    None  NaN</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 369);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну вот получается pyspark объект. Но я просто копирую тот же код, что и в обычном питоновском EDA. Это точно то что надо??? Просто скопировать тот же код, зачем?\n",
        "\n",
        "Ерунда какая-то..."
      ],
      "metadata": {
        "id": "7up5roIf2khK"
      }
    }
  ]
}